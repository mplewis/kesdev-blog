---
title: Designing Data-Intensive Applications Cheat Sheet
slug: designing-data-intensive-applications-cheat-sheet
date: 2022-08-04T22:06:05.000Z
---
<p>This is a summary I wrote of each chapter of the excellent <a href="https://amzn.to/3oSZjun">Designing Data-Intensive Applications (affiliate link)</a> book. I use this to prepare for remote job interviews in infrastructure and devops. Please enjoy!</p><h1 id="chapter-1">Chapter 1</h1><ul><li>Reliability: Systems must work correctly even when faults occur</li><li>Scalability: Systems have strategies for maintaining performance, even when load increases</li><li>Response time percentiles can measure performance</li><li>Maintainability: Makes life better for eng/ops teams who work sith the system</li><li>Good abstractions reduce complexity and make the system easier to modify and adapt</li><li>Good operability means visibility into system health and having ways to manage it</li></ul><h1 id="chapter-2">Chapter 2</h1><ul><li>Document DBs target use cases where data is self-contained and rarely related</li><li>Graph DBs target use cases where all data is deeply interlinked</li><li>Schema can be explicit (enforced on write) or implicit (enforced on read)</li></ul><h1 id="chapter-3">Chapter 3</h1><p><strong>OLTP:</strong> Optimized for transaction processing</p><ul><li>User-facing, large volume of requests</li><li>Small record count per query</li><li>Key-indexed lookup, dependent on disk seek time</li></ul><p><strong><strong><strong>Log-structured:</strong></strong></strong></p><ul><li>Append-only to files; delete obsolete files; do not update a written file</li><li>Random-access writes are turned into sequential writes on disk, enabling higher write throughput on HDD/SSD</li><li>LevelDB, Cassandra, HBase, Lucene</li><li><strong>Update-in-place: </strong>disk is a set of fixed-size pages that can be overwritten</li><li>B-trees, used in all major relational DBs</li></ul><p><strong>OLAP:</strong> Optimized for analytics processing</p><ul><li>Used by biz analysts, not end users</li><li>Lower query volume, but they demand millions of records scanned</li><li>Disk bandwidth is the bottleneck</li><li>One solution: column-oriented storage</li><li>Data warehouses: when your queries require sequential scans, indexes matter a lot less</li><li>It’s more important to encode data compactly to minimize the amount of data that a query must read from disk</li></ul><h1 id="chapter-4">Chapter 4</h1><ul><li>Rolling upgrades allow new versions of a service to be released without downtime</li><li>This promotes frequent small releases over rare big releases</li><li>This derisks deployments by allowing faulty releases to be rolled back before large user impact</li><li>This improves <strong>evolvability, </strong>the ease of making changes to an app</li><li>During rolling upgrades, different versions of the app are running at the same time</li><li>Encoding must be backward-compatible (new code reading old data) and forward-compatible (old code reading new data)</li><li>JSON, XML, CSV have optional schemas and are vague about datatypes (e.g. numbers)</li><li>Binary schema formats (Thrift, Protobuf, Avro, gRPC) provide compact, efficient encoding, with explicit forward- and backward-compatibility semantics, but are not human-readable</li></ul><h1 id="chapter-5">Chapter 5</h1><ul><li><strong>High availability:</strong> Keep system running even if 1+ machines goes down</li><li><strong>Disconnected operation:</strong> App keeps working even if network unavailable</li><li><strong>Latency: </strong>Place data closer to users so they can use it faster</li><li><strong>Scalability:</strong> Handle higher volume than any one machine could using read replicas</li></ul><p>Replication approaches:</p><ul><li><strong>Single-leader: </strong>Clients send all writes to a <strong>leader</strong> which streams data change events to <strong>followers; </strong>reads from followers may be stale.<br>Easy to understand, no conflict resolution</li><li><strong>Multi-leader: </strong>Clients send a write to any leader node; leaders stream events to each other and any followers</li><li><strong>Leaderless replication: </strong>Clients send each write to several nodes and read from several nodes in parallel to detect stale data.<br>Can be more robust to faulty nodes, network outage, latency, but harder to reason about, weak consistency guarantees</li></ul><p>Replication lag causes issues:</p><ul><li>Read-after-write: Users must always see data they submitted</li><li>Monotonic reads: Users must always see data in chronological order (not see earlier point-in-time data)</li><li>Consistent prefix reads: Users must always see data in a state that makes causal sense: a question is followed by its reply</li><li>In multi-leader and leaderless schemes, conflicts may occur and must be resolved</li></ul><h1 id="chapter-6">Chapter 6</h1><ul><li>Partitioning is necessary when data cannot fit onto a single machine</li><li>Goal is to spread data and query load evenly among multiple machines, avoiding hot spots</li><li>Must choose a partition scheme that fits data, and rebalance partitions when nodes are added or removed</li></ul><p>Approaches:</p><ul><li><strong><strong><strong>Key range partitioning:</strong> e.g. node owns keys from A through F</strong></strong></li><li>May cause hot spots if application frequently accesses keys that are close together in the sorted order</li><li><strong>Hash partitioning:</strong> keys are assigned to a node corresponding to their hashed value</li><li>Distributes load evenly but destroys ordering of keys, so range queries are inefficient</li><li>Common approach: Create a fixed number of partitions in advance, assign several to a node, and move entire partitions when a node is added/removed</li></ul><p>Secondary indices must also be partitioned:</p><ul><li><strong><strong><strong>Document-partitioned (local):</strong> secondary indices are stored in same partition as primary K/V</strong></strong></li><li>Only update a single partition on write; read of secondary index requires scatter/gather across all partitions</li><li><strong><strong><strong>Term-partitioned (global): </strong>secondary indices are partitioned separately using indexed values</strong></strong></li><li>Entry in secondary index may include records from all partitions of the primary key</li><li>Write updates several partitions; reads from a single partition</li></ul><h1 id="chapter-7">Chapter 7</h1><ul><li>Tranasactions allow an app to pretend that some concurrency problems and SW/HW faults don’t exist – lots of errors become “transaction aborts”</li><li>Txns hugely reduce the number of potential error cases you need to worry about</li><li>Without txns, hardware errors (power outage, disk crash) cause various data inconsistencies – hard to reason about effects of concurrent access</li></ul><p>Race conditions include:</p><ul><li>Dirty reads: Client sees another client’s writes before they are committed. Solvable with read-committed isolation level.</li><li>Dirty writes: Client overwrites another client’s data that has been written but not committed. Solvable with snapshot isolation.</li><li>Read skew: Client sees different parts of the DB at different points in time</li><li>Lost updates: Two clients perform a concurrent read-modify-write (e.g. bank balance problem). Solvable with snapshot isolation.</li><li>Write skew: Txn reads something, makes decision, writes decision – by the time the write is made, the premise is no longer true. Solvable with serializable isolation.</li><li>Phantom reads: Txn reads objects matching a search condition; someone else writes data that modifies those search results. Write skew issues may require index-range locks.</li></ul><p>Approaches to implementing serializable transactions:</p><ul><li>Literally executing transactions in serial order on a single CPU core</li><li>Two-phase locking: standard approach; may have poor performance</li><li>Serializable snapshot isolation: optimistically allow txns to proceed without blocking; commits are aborted if not serializable</li></ul><h1 id="chapter-8">Chapter 8</h1><p>Some kinds of partial failures in distributed systems:</p><ul><li>Network: Packets may be lost or artificially delayed</li><li>Time: Node clock may jump forward or backward and be out of sync with other nodes</li><li>Pauses: GC may pause a process; other nodes declare it dead; it resumes and is unaware it was paused</li><li>Any software that interacts with other nodes may fail, go slow, or time out</li></ul><p>Detecting faults:</p><ul><li>Most systems con’t know if a node has failed</li><li>Most distributed algorithms rely on timeouts to detect node failure</li><li>But timeouts might be network issues, not node failures</li><li>A limping node might cause more issues than a dead one</li><li>Once a fault is detected, info must flow over unreliable network between nodes – we rely on quorum protocols to make decisions</li><li>Distributed systems enable scalability, fault tolerance, and low latency</li></ul><h1 id="chapter-9">Chapter 9</h1><ul><li>Linearizability makes a database behave like an atomic variable, but is slow, especially across high-latency networks</li><li>Causality imposes an ordering on events, based on cause and effect</li><li>Weaker consistency model</li><li>Some things can be concurrent – branching and merging</li><li>Less sensitive to network issues than linearizability; lacks the coordination overhead of linearizability</li></ul><p><strong><strong><strong>Consensus</strong></strong></strong></p><ul><li>Solves atomic problems in causal models, e.g. signing up for a username requires that username to not already be taken</li><li>All nodes must agree on what was decided, irrevocably</li></ul><p>Consensus decision problems include:</p><ul><li>Linearizable compare-and-set registers: set a register based on a parameter</li><li>Atomic transaction commit: commit or abort</li><li>Total order broadcast: decide on order to deliver messages</li><li>Locks and leases: only one client can grab a lock</li><li>Membership/coordination: decide which nodes are alive and dead</li><li>Uniqueness constraint: which txn is allowed and failed due to constraint violation</li></ul><p>Single-leader failure resolutions:</p><ul><li>Wait for leader to recover</li><li>Human does manual failover</li><li>Algorithm chooses new leader</li><li>Even if a leader can be chosen algorithmically, we still need <strong>consensus </strong>to select the new leader</li><li>Leaderless and multi-leader replication systems don’t use global consensus</li></ul>